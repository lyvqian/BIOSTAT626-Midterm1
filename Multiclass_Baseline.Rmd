---
title: "Multiclass Baseline Algorithms"
author: "Leyuan Qian"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Load Packages

```{r}
library(dplyr)
library(caret)
library(randomForest)
library(ggplot2)
library(e1071)
library(ROCR)
library(pROC)
library(xgboost) 
library(neuralnet)
library(adabag)
library(MASS)
```

## Data Preprocessing

```{r}
training_data<-read.table("training_data.txt",header=T)
testing_data<-read.table("test_data.txt",header=T)

# Re-categorize the data
training_set=mutate(training_data,multi_activity=activity)
training_set$multi_activity[which(training_set$activity>6)]=7
training_set$multi_activity=as.factor(training_set$multi_activity)
training_set=training_set[,c(-1,-2)]
```

## Split Training Data for Hold-out Validation 

```{r}
set.seed(666)
n_subjects=nrow(training_set)
split_7_3 = sample(1:n_subjects,n_subjects*0.7)
train = training_set[split_7_3,]
test = training_set[-split_7_3,]
```

## Different Algorithms

### Linear Discriminant Analysis

```{r}
lda.fit = lda(multi_activity~., data = train)
lda.pred = predict(lda.fit, test)
lda.class = lda.pred$class
mean(lda.class==test$multi_activity) # accuracy
roc(test$multi_activity, lda.pred$posterior[,2]) # check auc
```

### kNN

```{r}
set.seed(601)
tgrid <- expand.grid(.k = seq(1, 73, by = 2))
tcontrol = trainControl(method = "cv")
knn.train <- train(multi_activity ~ ., data = train, method = "knn", trControl = tcontrol, 
    tuneGrid = tgrid) 
knn_pred = predict(knn.train,test)
mean(knn_pred==test$multi_activity) # accuracy
```
```{r}
mean(knn_pred==test$multi_activity) 
multiclass.roc(as.numeric(test$multi_activity),as.numeric(knn_pred)) # check auc
```

### Naive Bayes

```{r}
nb.train<- naiveBayes(multi_activity~., data=train)
nb_pred = predict(nb.train,test,prob=TRUE)
mean(nb_pred == test$multi_activity) # accuracy
multiclass.roc(test$multi_activity,as.numeric(nb_pred)) # check auc
```

### SVM with linear kernel

```{r}
set.seed(1)
tune.out <- tune(svm, multi_activity~.,data = train, kernel = "linear",ranges = list(cost = c(0.01, 0.1, 1, 10, 100)))
svm.fit=tune.out$best.model
svm.pred = predict(svm.fit, test)
mean(svm.pred==test$multi_activity) # accuracy
multiclass.roc(test$multi_activity,as.numeric(svm.pred)) # check auc
```

### SVM with radial kernel

```{r}
set.seed(515)
tune.out2 <- tune(svm, multi_activity~.,data = train, kernel = "radial",ranges = list(cost = c(0.01, 0.1, 1, 10, 100)))
svm.fit2=tune.out2$best.model
svm.pred2 = predict(svm.fit2, test)
mean(svm.pred2==test$multi_activity)  # accuracy
multiclass.roc(test$multi_activity,as.numeric(svm.pred2))  # check auc
```

### Random Forest

```{r}
set.seed(123)

# find best mtry parameter
best_mtry=0
error=1
for (i in 1:ceiling(sqrt(ncol(train)))){
  mtry_fit=randomForest(multi_activity~., data=train,mtry=i)
  err=mean(mtry_fit$err.rate)
  if (err<error){
     error=err
     best_mtry=i
  }
}

# find best ntree parameter
ntree_fit<-randomForest(multi_activity~.,data=train,mtry=best_mtry,ntree=1000)
plot(ntree_fit)
```

```{r}
rf=randomForest(multi_activity~., data=train, mtry=best_mtry, ntree=500,proximity = TRUE,
                              importance = TRUE)
rf_pred=predict(rf,test)
mean(rf_pred==test$multi_activity)
multiclass.roc(test$multi_activity,as.numeric(rf_pred))
```

### XGBoost

```{r}
set.seed(662)

# Prepare the correct data format
data4train=as.matrix(train[,-ncol(train)])
label4train=as.numeric(train[,ncol(train)])-1
data4test=as.matrix(test[,-ncol(test)])
label4test=as.numeric(test[,ncol(test)])-1
dtrain=xgb.DMatrix(data=data4train,label=label4train)
dtest=xgb.DMatrix(data=data4test,label=label4test)

# Cross Validation and model training
cv <- xgb.cv(data = dtrain, nrounds = 100, nfold = 10, metrics="merror",early_stopping_rounds=10,
max_depth = 7, eta = 0.6, objective = "multi:softmax",num_class=7)
xgbm<-xgboost(data = dtrain, nrounds =cv$best_iteration, early_stopping_rounds=10,max_depth = 7, eta = 0.6, objective = "multi:softmax",num_class=7)

pre_xgb<-round(predict(xgbm,newdata=dtest))
mean(pre_xgb==label4test)
multiclass.roc(label4test,pre_xgb)
```

### Feature Selection 1: Pearson Correlation

```{r}
correlationMatrix <- cor(train[,1:561])
highlyCorrelated <- findCorrelation(correlationMatrix, cutoff=0.8) 
train_cor<-train[,-highlyCorrelated] # remove features with correlation>0.8
```

```{r}
# kNN
set.seed(123)
tgrid_cor <- expand.grid(.k = seq(1,ceiling(sqrt(ncol(train_cor))), by = 1))
knn.train_cor <- train(multi_activity ~ ., data = train_cor, method = "knn", trControl = tcontrol, 
    tuneGrid = tgrid_cor)
knn_pred_cor = predict(knn.train_cor,test)
mean(knn_pred_cor==test$multi_activity)
multiclass.roc(test$multi_activity,as.numeric(knn_pred_cor))
```

```{r}
# SVM with linear kernel
set.seed(6)
tune.out3 <- tune(svm, multi_activity~.,data = train_cor, kernel = "linear",ranges = list(cost = c(0.01, 0.1, 1, 10, 100)))
svm.fit3=tune.out3$best.model
svm.pred3 = predict(svm.fit3, test)
mean(svm.pred3==test$multi_activity)
multiclass.roc(test$multi_activity,as.numeric(svm.pred3))
```

```{r}
# SVM with radial kernel
set.seed(515)
tune.out4 <- tune(svm, multi_activity~.,data = train_cor , kernel = "radial",ranges = list(cost = c(0.01, 0.1, 1, 10, 100)))
svm.fit4=tune.out4$best.model
svm.pred4 = predict(svm.fit4, test)
mean(svm.pred4==test$multi_activity)
multiclass.roc(test$multi_activity,as.numeric(svm.pred4))
```

```{r}
# XGBoost
data4train_cor=as.matrix(train_cor[,-ncol(train_cor)])
label4train_cor=as.numeric(train_cor[,ncol(train_cor)])-1
dtrain_cor=xgb.DMatrix(data=data4train_cor,label=label4train_cor)
test_cor=subset(test,select=colnames(train_cor))
data4test_cor=as.matrix(test_cor[,-ncol(test_cor)])
label4test_cor=as.numeric(test_cor[,ncol(test_cor)])-1
dtest_cor=xgb.DMatrix(data=data4test_cor,label=label4test_cor)

cv_cor <- xgb.cv(data = dtrain_cor, nrounds = 20, nfold = 5,
                 metrics="merror",early_stopping_rounds=8,max_depth = 7, eta = 0.6, 
                 objective ="multi:softmax",num_class=7)
xgbm_cor<-xgboost(data = dtrain_cor, nrounds =cv_cor$best_iteration, nfold = 5,
                  metrics="merror",early_stopping_rounds=8,
                  max_depth = 7, eta = 0.6, objective = "multi:softmax",num_class=7)

pre_xgb_cor<-round(predict(xgbm_cor,newdata=dtest_cor))
mean(pre_xgb_cor==label4test_cor)
multiclass.roc(label4test_cor,pre_xgb_cor)
```

### Feature Selection 2: Random Forest

```{r}
importance_rf <- as.data.frame(rf$importance)
importance_rf<-importance_rf[order(importance_rf$MeanDecreaseAccuracy, decreasing = TRUE), ]
set.seed(100)
rf.cv <- rfcv(train[-ncol(train)], train$multi_activity, cv.fold = 10, step = 1.5)
rf.cv #find a reasonable number of predictors to keep
```

```{r}
# make train and test set for rf-selected predictors 
importance.rf.select <- importance_rf[1:249, ]
predictor.rf.select <- rownames(importance.rf.select)
training_set.rf.select<-training_set[,c(predictor.rf.select,"multi_activity")]

set.seed(123)
rndsample <- sample(nrow(training_set.rf.select), nrow(training_set.rf.select)*0.7)
train.rf.select <- training_set.rf.select[rndsample, ]
test.rf.select <- training_set.rf.select[-rndsample, ]
```

```{r}
# Random Forest
set.seed(333)

# find best mtry parameter
best_mtry2=0
error2=1
for (i in 1:ceiling(sqrt(ncol(train.rf.select)))){
  mtry_fit=randomForest(multi_activity~., data=train.rf.select,mtry=i)
  err=mean(mtry_fit$err.rate)
  if (err<error2){
     error2=err
     best_mtry2=i
  }
}

# find best ntree parameter
ntree_fit2<-randomForest(multi_activity~.,data=train.rf.select,mtry=best_mtry2,ntree=1000)
plot(ntree_fit2)
```

```{r}
# random forest for rf-selected predictors 
rf.select <- randomForest(multi_activity~., data = train.rf.select, mtry=best_mtry2, ntree=400, proximity = TRUE, importance = TRUE)
pred.rf.select <- predict(rf.select, test.rf.select)
mean(pred.rf.select==test.rf.select$multi_activity)
multiclass.roc(test.rf.select$multi_activity,as.numeric(pred.rf.select))
```

```{r}
# XGBoost
set.seed(233)

data4train_rf=as.matrix(train.rf.select[,-ncol(train.rf.select)])
label4train_rf=as.numeric(train.rf.select[,ncol(train.rf.select)])-1
data4test_rf=as.matrix(test.rf.select[,-ncol(test.rf.select)])
label4test_rf=as.numeric(test.rf.select[,ncol(test.rf.select)])-1
dtrain_rf=xgb.DMatrix(data=data4train_rf,label=label4train_rf)
dtest_rf=xgb.DMatrix(data=data4test_rf,label=label4test_rf)

cv_rf <- xgb.cv(data = dtrain_rf, nrounds = 100, nfold = 10, metrics="merror",
                early_stopping_rounds=10, max_depth = 4, eta = 0.5, 
                objective = "multi:softmax",num_class=7) 
xgbm_rf<-xgboost(data = dtrain_rf, nrounds =cv_rf$best_iteration, 
               early_stopping_rounds=10, max_depth = 4, eta = 0.5, 
               objective = "multi:softmax",num_class=7)

pre_xgb_rf<-round(predict(xgbm_rf,newdata=dtest_rf))
mean(pre_xgb_rf==label4test_rf)
multiclass.roc(label4test_rf,pre_xgb_rf)
```

### Feature Selection 3: XGBoost

```{r}
importance_xgb <- xgb.importance(colnames(data4train), model = xgbm)  
top_importance_xgb<-importance_xgb[which(importance_xgb$Gain>0.001)]
xgb.ggplot.importance(importance_xgb)
xgb.ggplot.importance(top_importance_xgb)
```

```{r}
set.seed(120)

feature.select.xgb <- top_importance_xgb$Feature
training_data.xgb=training_set[,c(feature.select.xgb,"multi_activity")]
training_data.xgb$multi_activity=as.factor(training_data.xgb$multi_activity)
n_subjects.xgb=nrow(training_data)
rndsp = sample(1:n_subjects.xgb,n_subjects.xgb*0.7)
train.xgb = training_data.xgb[rndsp,]
test.xgb = training_data.xgb[-rndsp,]

data4train.xgb=as.matrix(train.xgb[,-ncol(train.xgb)])
label4train.xgb=as.numeric(train.xgb[,ncol(train.xgb)])-1
data4test.xgb=as.matrix(test.xgb[,-ncol(test.xgb)])
label4test.xgb=as.numeric(test.xgb[,ncol(test.xgb)])-1
dtrain.xgb=xgb.DMatrix(data=data4train.xgb,label=label4train.xgb)
dtest.xgb=xgb.DMatrix(data=data4test.xgb,label=label4test.xgb)

cv.xgb <- xgb.cv(data = dtrain.xgb, nrounds = 100, nfold = 10,
                 metrics="merror",early_stopping_rounds=8,max_depth = 7, 
                 eta = 0.6, objective = "multi:softmax",num_class=7)
xgbm.xgb<-xgboost(data = dtrain.xgb, nrounds =cv.xgb$best_iteration, 
                  early_stopping_rounds=8,max_depth = 7, eta = 0.6, 
                  objective = "multi:softmax",num_class=7)
xgb_pre.xgb<-round(predict(xgbm.xgb,newdata=dtest.xgb))
mean(xgb_pre.xgb==label4test.xgb)
multiclass.roc(xgb_pre.xgb,label4test.xgb)
```

